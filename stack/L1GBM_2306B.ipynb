{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics#, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import sys\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "path = \"/home/darragh/avito/data/\"\n",
    "#path = '/Users/dhanley2/Documents/avito/data/'\n",
    "#path = '/home/ubuntu/avito/data/'\n",
    "#data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb25 = pd.read_csv('../lgCV_2505.csv.gz', compression='gzip')\n",
    "lgb02A = pd.read_csv(path+'../sub/lgCV_0206A.csv.gz', compression='gzip')\n",
    "lgb09 = pd.read_csv(path+'../sub/lgCV_0906.csv.gz', compression='gzip')\n",
    "lgb10 = pd.read_csv(path+'../sub/lgCV_1006.csv.gz', compression='gzip')\n",
    "lgb11A= pd.read_csv(path+'../sub/lgCV_1106A.csv.gz', compression='gzip')\n",
    "lgb11D= pd.read_csv(path+'../sub/lgCV_1106D.csv.gz', compression='gzip')\n",
    "lgb14= pd.read_csv(path+'../sub/lgCV_1406.csv.gz', compression='gzip')\n",
    "lgb14A= pd.read_csv(path+'../sub/lgCV_1406A.csv.gz', compression='gzip')\n",
    "lgb27 = pd.read_csv(path+'../sub/lgCV_2705B.csv.gz', compression='gzip')\n",
    "lgb31 = pd.read_csv(path+'../sub/lgCV_3105.csv.gz', compression='gzip')\n",
    "lgb02 = pd.read_csv(path+'../sub/lgCV_0206.csv.gz', compression='gzip')\n",
    "lgb17 = pd.read_csv(path+'../sub/lgCV_1706.csv.gz', compression='gzip')\n",
    "lgb19 = pd.read_csv(path+'../sub/lgCV_1906.csv.gz', compression='gzip')\n",
    "lgb20 = pd.read_csv(path+'../sub/lgCV_2006.csv.gz', compression='gzip')\n",
    "lgb20C = pd.read_csv(path+'../sub/lgCV_2006C.csv.gz', compression='gzip')\n",
    "lgb20B = pd.read_csv(path+'../sub/lgCV_2006B.csv.gz', compression='gzip')\n",
    "lgb20AA = pd.read_csv(path+'../sub/lgCV_2006AA.csv.gz', compression='gzip')\n",
    "lgb19A= pd.read_csv(path+'../sub/lgCV_1906A.csv.gz', compression='gzip')\n",
    "rnn =   pd.read_csv(path+'../sub/rnnCV_2805.csv.gz', compression='gzip')\n",
    "rnn27 = pd.read_csv(path+'../sub/rnnCV_2705A.csv.gz', compression='gzip')\n",
    "rnn12 = pd.read_csv(path+'../sub/rnnCV_1206.csv.gz', compression='gzip')\n",
    "rnn12 = pd.read_csv(path+'../sub/rnnCV_1206.csv.gz', compression='gzip')\n",
    "mlp =   pd.read_csv(path+'../sub/mlpCV_2505.csv.gz', compression='gzip')\n",
    "rmlp23A =   pd.read_csv(path+'../sub/rmlp5CV_2306A.csv.gz', compression='gzip')\n",
    "rmlp23 =   pd.read_csv(path+'../sub/rmlp5CV_2306.csv.gz', compression='gzip')\n",
    "rdgv19 =   pd.read_csv(path+'../sub/rdgv19CV_1606.csv.gz', compression='gzip')\n",
    "rdgv20 =   pd.read_csv(path+'../sub/rdg5CV_2006B.csv.gz', compression='gzip')\n",
    "rdgv23 =   pd.read_csv(path+'../sub/rdg5CV_2306.csv.gz', compression='gzip')\n",
    "rdgv23A =   pd.read_csv(path+'../sub/rdg5CV_2306A.csv.gz', compression='gzip')\n",
    "\n",
    "\n",
    "truth = pd.read_csv(path+'train.csv.zip', compression='zip', parse_dates = [\"activation_date\"])\n",
    "y =     truth['deal_probability'].values\n",
    "truth.drop('deal_probability', 1)\n",
    "test =  pd.read_csv(path+'test.csv.zip', compression='zip', parse_dates = [\"activation_date\"])\n",
    "test['deal_probability']=float('NAN') \n",
    "truth = pd.concat([truth,test[truth.columns]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth['image_on_ad'] = ((truth['image_top_1']==truth['image_top_1']).astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.read_csv(path+'../features/user_entropy_2306.gz', compression='gzip')\n",
    "entropy.drop(['user_id', 'index'], 1, inplace = True)\n",
    "entropy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb14['deal_probability'] =  ( lgb14['deal_probability'].values + lgb14A['deal_probability'].values)*0.5\n",
    "lgb17['deal_probability'] =  ( lgb17['deal_probability'].values + lgb19['deal_probability'].values)*0.25\n",
    "lgb20['deal_probability'] =  ( lgb20['deal_probability'].values) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_cts = truth['image_top_1'].value_counts().reset_index()\n",
    "#truth['image_top_big'] = 'lo_count'\n",
    "#big_top_1 = img_cts[img_cts['image_top_1']>5000]['index'].tolist()\n",
    "#idx = truth['image_top_1'].isin(big_top_1)\n",
    "#truth['image_top_big'][idx] = truth['image_top_1'][idx].astype(str).values\n",
    "\n",
    "def keep_big(df, col, cutoff):\n",
    "    cts = df[col].value_counts().reset_index()\n",
    "    df[col+'_big'] = 'lo_count'\n",
    "    big = cts[cts[col]>cutoff]['index'].tolist()\n",
    "    idx = df[col].isin(big)\n",
    "    df[col+'_big'][idx] = df[col][idx].astype(str).values\n",
    "    return df[col+'_big'].values\n",
    "truth['image_top_1_big'] = keep_big(truth, 'image_top_1', 5000)\n",
    "truth['param_1_big'] = keep_big(truth, 'param_1', 5000)\n",
    "truth['param_2_big'] = keep_big(truth, 'param_2', 5000)\n",
    "truth['param_3_big'] = keep_big(truth, 'param_3', 5000)\n",
    "truth['city_big'] = keep_big(truth, 'city', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth[[c for c in truth.columns if 'big' in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb25.rename(columns={'deal_probability': 'lgb25_preds' }, inplace=True)\n",
    "lgb31.rename(columns={'deal_probability': 'lgb31_preds' }, inplace=True)\n",
    "lgb27.rename(columns={'deal_probability': 'lgb27_preds' }, inplace=True)\n",
    "lgb02.rename(columns={'deal_probability': 'lgb02_preds' }, inplace=True)\n",
    "lgb09.rename(columns={'deal_probability': 'lgb09_preds' }, inplace=True)\n",
    "lgb10.rename(columns={'deal_probability': 'lgb10_preds' }, inplace=True)\n",
    "lgb11D.rename(columns={'deal_probability': 'lgb11D_preds' }, inplace=True)\n",
    "lgb11A.rename(columns={'deal_probability': 'lgb11A_preds' }, inplace=True)\n",
    "lgb14.rename(columns={'deal_probability': 'lgb14_preds' }, inplace=True)\n",
    "lgb17.rename(columns={'deal_probability': 'lgb17_preds' }, inplace=True)\n",
    "lgb20.rename(columns={'deal_probability': 'lgb20_preds' }, inplace=True)\n",
    "lgb20C.rename(columns={'deal_probability': 'lgb20C_preds' }, inplace=True)\n",
    "lgb20B.rename(columns={'deal_probability': 'lgb20B_preds' }, inplace=True)\n",
    "lgb20AA.rename(columns={'deal_probability': 'lgb20AA_preds' }, inplace=True)\n",
    "lgb19A.rename(columns={'deal_probability': 'lgb19A_preds' }, inplace=True)\n",
    "lgb02A.rename(columns={'deal_probability': 'lgb02A_preds' }, inplace=True)\n",
    "rnn27.rename(columns={'deal_probability': 'rnn27_preds' }, inplace=True)\n",
    "rnn12.rename(columns={'deal_probability': 'rnn12_preds' }, inplace=True)\n",
    "rdgv19.rename(columns={'deal_probability': 'rdgv19_preds' }, inplace=True)\n",
    "rdgv20.rename(columns={'deal_probability': 'rdgv20_preds' }, inplace=True)\n",
    "rdgv23.rename(columns={'deal_probability': 'rdgv23_preds' }, inplace=True)\n",
    "rdgv23A.rename(columns={'deal_probability': 'rdgv23A_preds' }, inplace=True)\n",
    "rmlp23.rename(columns={'deal_probability': 'rmlp23_preds' }, inplace=True)\n",
    "rmlp23A.rename(columns={'deal_probability': 'rmlp23A_preds' }, inplace=True)\n",
    "mlp.rename(columns={'deal_probability': 'mlp_preds' }, inplace=True)\n",
    "preds_df = lgb27.merge(rnn, on='item_id')\\\n",
    "                .merge(mlp, on='item_id')\\\n",
    "                .merge(rmlp23, on='item_id')\\\n",
    "                .merge(rmlp23A, on='item_id')\\\n",
    "                .merge(lgb31, on='item_id')\\\n",
    "                .merge(lgb02, on='item_id')\\\n",
    "                .merge(lgb09, on='item_id')\\\n",
    "                .merge(lgb10, on='item_id')\\\n",
    "                .merge(lgb11A, on='item_id')\\\n",
    "                .merge(lgb11D, on='item_id')\\\n",
    "                .merge(lgb14, on='item_id')\\\n",
    "                .merge(lgb17, on='item_id')\\\n",
    "                .merge(lgb20, on='item_id')\\\n",
    "                .merge(lgb20C, on='item_id')\\\n",
    "                .merge(lgb20B, on='item_id')\\\n",
    "                .merge(lgb20AA, on='item_id')\\\n",
    "                .merge(lgb19A, on='item_id')\\\n",
    "                .merge(lgb02A, on='item_id')\\\n",
    "                .merge(rnn27, on='item_id')\\\n",
    "                .merge(rnn12, on='item_id')\\\n",
    "                .merge(rdgv19, on='item_id')\\\n",
    "                .merge(rdgv20, on='item_id')\\\n",
    "                .merge(rdgv23, on='item_id')\\\n",
    "                .merge(rdgv23A, on='item_id')\\\n",
    "                .merge(truth, on='item_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rnn, \\\n",
    "                mlp, \\\n",
    "                rmlp23, \\\n",
    "                rmlp23A, \\\n",
    "                lgb31, \\\n",
    "                lgb02, \\\n",
    "                lgb09, \\\n",
    "                lgb10, \\\n",
    "                lgb11A, \\\n",
    "                lgb11D, \\\n",
    "                lgb14, \\\n",
    "                lgb17, \\\n",
    "                lgb20, \\\n",
    "                lgb20C, \\\n",
    "                lgb20B, \\\n",
    "                lgb20AA, \\\n",
    "                lgb19A, \\\n",
    "                lgb02A, \\\n",
    "                rnn27, \\\n",
    "                rnn12, \\\n",
    "                rdgv19, \\\n",
    "                rdgv20, \\\n",
    "                rdgv23, \\\n",
    "                rdgv23A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in preds_df.columns if ('_preds' in col) \\\n",
    "             and ('lgb27' not in col) and ('lgb02_' not in col) and ('lgb11A_' not in col) \\\n",
    "             and ('rdgv19_' not in col) and ('lgb17_' not in col)]\n",
    "\n",
    "preds_df['preds_sum'] = preds_df[pred_cols].sum(axis=1)\n",
    "pred_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df['price'].fillna(-1,inplace=True)\n",
    "preds_df['_max_'] = np.max(np.array([preds_df[col] for col in pred_cols]),axis=0).astype(np.float32)\n",
    "preds_df['_min_'] = np.min(np.array([preds_df[col] for col in pred_cols]),axis=0).astype(np.float32)\n",
    "preds_df['_avg_'] = np.mean(np.array([preds_df[col] for col in pred_cols]),axis=0).astype(np.float32)\n",
    "preds_df['_med_'] = np.median(np.array([preds_df[col] for col in pred_cols]),axis=0).astype(np.float32)\n",
    "preds_df['_std_'] = np.std(np.array([preds_df[col] for col in pred_cols]),axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for v in ['rnn', 'rdg', 'lgb1', 'lgb2']:\n",
    "#    cols = [col for col in preds_df.columns if v in col]\n",
    "#    preds_df[v+'_max_'] = np.max(np.array([preds_df[col] for col in cols]),axis=0).astype(np.float32)\n",
    "#    preds_df[v+'_min_'] = np.min(np.array([preds_df[col] for col in cols]),axis=0).astype(np.float32)\n",
    "#    preds_df[v+'_avg_'] = np.mean(np.array([preds_df[col] for col in cols]),axis=0).astype(np.float32)\n",
    "#    preds_df[v+'_med_'] = np.median(np.array([preds_df[col] for col in cols]),axis=0).astype(np.float32)\n",
    "#    preds_df[v+'_std_'] = np.std(np.array([preds_df[col] for col in cols]),axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for p1, p2 in itertools.combinations(pred_cols, 2):\n",
    "    print('difference_%s__%s'%(p1,p2))\n",
    "    preds_df['difference_%s__%s'%(p1,p2)] = (preds_df[p2] - preds_df[p1]).astype(np.float32)\n",
    "    preds_df['sums_%s__%s'%(p1,p2)] = (preds_df[p2] + preds_df[p1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in entropy.columns:\n",
    "    print(col)\n",
    "    preds_df[col] = (entropy[col].values).astype(np.float32)\n",
    "del entropy\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in preds_df.columns if ('preds' in col) \n",
    "             and ('difference' not in col) \n",
    "             and ('sum' not in col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = preds_df['deal_probability']==preds_df['deal_probability']\n",
    "print(idx.value_counts())\n",
    "for col in [c for c in preds_df.columns if ('_preds' in c) and ('difference' not in c) and ('sum' not in c)]:\n",
    "    print('RMSE %s: '%(col), np.sqrt(metrics.mean_squared_error(preds_df['deal_probability'][idx].values, preds_df[col][idx].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldls = [[\"2017-03-15\", \"2017-03-16\", \"2017-03-17\"], \\\n",
    "       [\"2017-03-18\", \"2017-03-19\", \"2017-03-20\"], \\\n",
    "       [\"2017-03-21\", \"2017-03-22\", \"2017-03-23\"], \\\n",
    "       [\"2017-03-24\", \"2017-03-25\", \"2017-03-26\"], \\\n",
    "        [\"2017-03-27\", \"2017-03-28\", \"2017-03-29\", \\\n",
    "            \"2017-03-30\", \"2017-03-31\", \"2017-04-01\", \\\n",
    "            \"2017-04-02\", \"2017-04-03\",\"2017-04-07\"]]\n",
    "foldls = [[pd.to_datetime(d) for d in f] for f in foldls]\n",
    "preds_df['fold'] = -1\n",
    "for t, fold in enumerate(foldls):\n",
    "    preds_df['fold'][preds_df.activation_date.isin(fold)] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for col in [c for c in preds_df.columns if ('_preds' in c) \\\n",
    "            and ('difference' not in c) \\\n",
    "            and ('sum' not in c)]:\n",
    "    lstmp = [col]\n",
    "    for i in range(5):\n",
    "        idx = preds_df['fold']==i\n",
    "        lstmp.append(np.sqrt(metrics.mean_squared_error(preds_df['deal_probability'][idx].values, \\\n",
    "                                                        preds_df[col][idx].values)))\n",
    "    scores.append(lstmp)\n",
    "pd.DataFrame(scores, columns = ['Model']+['Fold%s'%(i) for i in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations in test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correlation\n",
    "preds_df[~preds_df['deal_probability'].isnull()][[c for c in preds_df.columns if ('_preds' in c) \\\n",
    "                                                  and ('difference' not in c) and ('sum' not in c) ]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train correlation\n",
    "preds_df[preds_df['deal_probability'].isnull()][[c for c in preds_df.columns if ('_preds' in c)  \\\n",
    "                                                  and ('difference' not in c) and ('sum' not in c) ]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in preds_df.columns if '_preds' in c]\n",
    "cols += [c for c in preds_df.columns if 'difference' in c]\n",
    "cols += [c for c in preds_df.columns if '_entropy' in c]\n",
    "cols += [c for c in preds_df.columns if 'imgnet' in c]\n",
    "cols += [c for c in preds_df.columns if 'image_on_ad' in c]\n",
    "cols += ['price', 'item_seq_number']\n",
    "for ii in ['_max_', '_min_', '_avg_', '_std_', '_med_']:\n",
    "    cols += [c for c in preds_df.columns if ii in c]\n",
    "categories = ['region', 'param_1_big', 'parent_category_name', 'category_name', \\\n",
    "              'param_2_big', 'param_3_big', 'city_big', 'user_type', 'image_top_1_big']#,\n",
    "cols += categories\n",
    "cols = list(set(cols))\n",
    "print(len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categories:\n",
    "    preds_df[col] = LabelEncoder().fit_transform(preds_df[col].fillna(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preds_df[~preds_df['deal_probability'].isnull()]\n",
    "test_df = preds_df[preds_df['deal_probability'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 4000\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_df[cols], y, train_size=.8, random_state=12345)\n",
    "eval_set = [(train_X,train_y),(valid_X,valid_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "clf = LGBMRegressor(n_estimators=n_estimators, \n",
    "                    max_depth=-1, \n",
    "                    feature_fraction= 0.4,\n",
    "                    num_leaves=32, \n",
    "                    learning_rate=.01)#, device='gpu')\n",
    "clf.fit(train_X, train_y, early_stopping_rounds=80, \n",
    "        eval_set=eval_set, eval_metric='rmse', verbose=100, \n",
    "        categorical_feature=categories)\n",
    "# [2683]\ttraining's rmse: 0.207184\tvalid_1's rmse: 0.211402\n",
    "# [2551]\ttraining's rmse: 0.207177\tvalid_1's rmse: 0.211261\n",
    "# [2810]\ttraining's rmse: 0.206681\tvalid_1's rmse: 0.211139\n",
    "\n",
    "# [3046]\ttraining's rmse: 0.206266\tvalid_1's rmse: 0.211108\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 80 rounds.\n",
    "[100]\ttraining's rmse: 0.218692\tvalid_1's rmse: 0.219082\n",
    "[200]\ttraining's rmse: 0.212113\tvalid_1's rmse: 0.212733\n",
    "[300]\ttraining's rmse: 0.210908\tvalid_1's rmse: 0.211734\n",
    "[400]\ttraining's rmse: 0.210495\tvalid_1's rmse: 0.211507\n",
    "[500]\ttraining's rmse: 0.210227\tvalid_1's rmse: 0.211409\n",
    "[600]\ttraining's rmse: 0.210001\tvalid_1's rmse: 0.211346\n",
    "[700]\ttraining's rmse: 0.209801\tvalid_1's rmse: 0.211306\n",
    "[800]\ttraining's rmse: 0.209616\tvalid_1's rmse: 0.211277\n",
    "[900]\ttraining's rmse: 0.209444\tvalid_1's rmse: 0.211255\n",
    "[1000]\ttraining's rmse: 0.209277\tvalid_1's rmse: 0.211238\n",
    "[1100]\ttraining's rmse: 0.209116\tvalid_1's rmse: 0.211222\n",
    "[1200]\ttraining's rmse: 0.208959\tvalid_1's rmse: 0.211207\n",
    "[1300]\ttraining's rmse: 0.208809\tvalid_1's rmse: 0.211197\n",
    "[1400]\ttraining's rmse: 0.208659\tvalid_1's rmse: 0.211187\n",
    "[1500]\ttraining's rmse: 0.208512\tvalid_1's rmse: 0.211181\n",
    "[1600]\ttraining's rmse: 0.208368\tvalid_1's rmse: 0.211174\n",
    "[1700]\ttraining's rmse: 0.208221\tvalid_1's rmse: 0.211165\n",
    "[1800]\ttraining's rmse: 0.208076\tvalid_1's rmse: 0.211161\n",
    "[1900]\ttraining's rmse: 0.207931\tvalid_1's rmse: 0.211153\n",
    "[2000]\ttraining's rmse: 0.207787\tvalid_1's rmse: 0.211148\n",
    "[2100]\ttraining's rmse: 0.207646\tvalid_1's rmse: 0.211145\n",
    "[2200]\ttraining's rmse: 0.207504\tvalid_1's rmse: 0.211139\n",
    "[2300]\ttraining's rmse: 0.207366\tvalid_1's rmse: 0.211137\n",
    "[2400]\ttraining's rmse: 0.207227\tvalid_1's rmse: 0.211136\n",
    "[2500]\ttraining's rmse: 0.207088\tvalid_1's rmse: 0.211133\n",
    "[2600]\ttraining's rmse: 0.206951\tvalid_1's rmse: 0.211131\n",
    "[2700]\ttraining's rmse: 0.206818\tvalid_1's rmse: 0.211128\n",
    "[2800]\ttraining's rmse: 0.206682\tvalid_1's rmse: 0.211123\n",
    "Early stopping, best iteration is:\n",
    "[2793]\ttraining's rmse: 0.206691\tvalid_1's rmse: 0.211123\n",
    "CPU times: user 2h 3min 7s, sys: 29.1 s, total: 2h 3min 36s\n",
    "Wall time: 16min 6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(clf.feature_importances_, train_X.columns ),key=lambda x: -x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 2800\n",
    "train_X = train_df[cols]\n",
    "train_y = y\n",
    "eval_set = [(train_X,train_y)]\n",
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_predls = []\n",
    "for i in range(5):    \n",
    "    del clf\n",
    "    gc.collect()\n",
    "    clf = LGBMRegressor(n_estimators=n_estimators, \n",
    "                    max_depth=-1, \n",
    "                    feature_fraction= 0.4,\n",
    "                    num_leaves=32, \n",
    "                    seed = i, \n",
    "                    learning_rate=.01)#, device='gpu')\n",
    "    clf.fit(train_X, train_y, early_stopping_rounds=80, \n",
    "        eval_set=eval_set, eval_metric='rmse', verbose=100, \n",
    "        categorical_feature=categories)\n",
    "    y_predls.append(clf.predict(test_df[cols]))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['deal_probability'] = sum(y_predls)/len(y_predls)\n",
    "test_df['deal_probability'] = np.clip(test_df['deal_probability'], .0001, .9999)\n",
    "test_df[['item_id', 'deal_probability']].to_csv('../lgbbsub_2306AL2.csv.gz', compression='gzip', index=False, header=True)\n",
    "test_df[['item_id', 'deal_probability']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
